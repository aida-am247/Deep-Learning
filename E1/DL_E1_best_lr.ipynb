{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_E1_best_lr.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOLIoh/sFmcQo8RmaziAgYo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aida-am247/Deep-Learning/blob/master/E1/DL_E1_best_lr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L2wEKO-zHgCX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "outputId": "88e052d2-0387-4099-81ae-c506bb6617f1"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/Alireza-Akhavan/SRU-deeplearning-workshop/master/dataset.py\n",
        "!mkdir dataset\n",
        "!wget https://github.com/Alireza-Akhavan/SRU-deeplearning-workshop/raw/master/dataset/Data_hoda_full.mat -P dataset"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-22 15:09:33--  https://raw.githubusercontent.com/Alireza-Akhavan/SRU-deeplearning-workshop/master/dataset.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 929 [text/plain]\n",
            "Saving to: ‘dataset.py’\n",
            "\n",
            "\rdataset.py            0%[                    ]       0  --.-KB/s               \rdataset.py          100%[===================>]     929  --.-KB/s    in 0s      \n",
            "\n",
            "2020-09-22 15:09:33 (52.0 MB/s) - ‘dataset.py’ saved [929/929]\n",
            "\n",
            "--2020-09-22 15:09:34--  https://github.com/Alireza-Akhavan/SRU-deeplearning-workshop/raw/master/dataset/Data_hoda_full.mat\n",
            "Resolving github.com (github.com)... 140.82.118.4\n",
            "Connecting to github.com (github.com)|140.82.118.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Alireza-Akhavan/SRU-deeplearning-workshop/master/dataset/Data_hoda_full.mat [following]\n",
            "--2020-09-22 15:09:34--  https://raw.githubusercontent.com/Alireza-Akhavan/SRU-deeplearning-workshop/master/dataset/Data_hoda_full.mat\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3989009 (3.8M) [application/octet-stream]\n",
            "Saving to: ‘dataset/Data_hoda_full.mat’\n",
            "\n",
            "Data_hoda_full.mat  100%[===================>]   3.80M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-09-22 15:09:35 (34.7 MB/s) - ‘dataset/Data_hoda_full.mat’ saved [3989009/3989009]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OfJsXzgOvxF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the necessary packages\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tempfile\n",
        "\n",
        "class LearningRateFinder:\n",
        "\tdef __init__(self, model, stopFactor=4, beta=0.98):\n",
        "\t\t# store the model, stop factor, and beta value (for computing\n",
        "\t\t# a smoothed, average loss)\n",
        "\t\tself.model = model\n",
        "\t\tself.stopFactor = stopFactor\n",
        "\t\tself.beta = beta\n",
        "\n",
        "\t\t# initialize our list of learning rates and losses,\n",
        "\t\t# respectively\n",
        "\t\tself.lrs = []\n",
        "\t\tself.losses = []\n",
        "\n",
        "\t\t# initialize our learning rate multiplier, average loss, best\n",
        "\t\t# loss found thus far, current batch number, and weights file\n",
        "\t\tself.lrMult = 1\n",
        "\t\tself.avgLoss = 0\n",
        "\t\tself.bestLoss = 1e9\n",
        "\t\tself.batchNum = 0\n",
        "\t\tself.weightsFile = None\n",
        "\n",
        "\tdef reset(self):\n",
        "\t\t# re-initialize all variables from our constructor\n",
        "\t\tself.lrs = []\n",
        "\t\tself.losses = []\n",
        "\t\tself.lrMult = 1\n",
        "\t\tself.avgLoss = 0\n",
        "\t\tself.bestLoss = 1e9\n",
        "\t\tself.batchNum = 0\n",
        "\t\tself.weightsFile = None\n",
        "\n",
        "\tdef is_data_iter(self, data):\n",
        "\t\t# define the set of class types we will check for\n",
        "\t\titerClasses = [\"NumpyArrayIterator\", \"DirectoryIterator\",\n",
        "\t\t\t \"Iterator\", \"Sequence\"]\n",
        "\n",
        "\t\t# return whether our data is an iterator\n",
        "\t\treturn data.__class__.__name__ in iterClasses\n",
        "\n",
        "\tdef on_batch_end(self, batch, logs):\n",
        "\t\t# grab the current learning rate and add log it to the list of\n",
        "\t\t# learning rates that we've tried\n",
        "\t\tlr = K.get_value(self.model.optimizer.lr)\n",
        "\t\tself.lrs.append(lr)\n",
        "\n",
        "\t\t# grab the loss at the end of this batch, increment the total\n",
        "\t\t# number of batches processed, compute the average average\n",
        "\t\t# loss, smooth it, and update the losses list with the\n",
        "\t\t# smoothed value\n",
        "\t\tl = logs[\"loss\"]\n",
        "\t\tself.batchNum += 1\n",
        "\t\tself.avgLoss = (self.beta * self.avgLoss) + ((1 - self.beta) * l)\n",
        "\t\tsmooth = self.avgLoss / (1 - (self.beta ** self.batchNum))\n",
        "\t\tself.losses.append(smooth)\n",
        "\n",
        "\t\t# compute the maximum loss stopping factor value\n",
        "\t\tstopLoss = self.stopFactor * self.bestLoss\n",
        "\n",
        "\t\t# check to see whether the loss has grown too large\n",
        "\t\tif self.batchNum > 1 and smooth > stopLoss:\n",
        "\t\t\t# stop returning and return from the method\n",
        "\t\t\tself.model.stop_training = True\n",
        "\t\t\treturn\n",
        "\n",
        "\t\t# check to see if the best loss should be updated\n",
        "\t\tif self.batchNum == 1 or smooth < self.bestLoss:\n",
        "\t\t\tself.bestLoss = smooth\n",
        "\n",
        "\t\t# increase the learning rate\n",
        "\t\tlr *= self.lrMult\n",
        "\t\tK.set_value(self.model.optimizer.lr, lr)\n",
        "\n",
        "\tdef find(self, trainData, startLR, endLR, epochs=None,\n",
        "\t\tstepsPerEpoch=None, batchSize=32, sampleSize=2048,\n",
        "\t\tverbose=1):\n",
        "\t\t# reset our class-specific variables\n",
        "\t\tself.reset()\n",
        "\n",
        "\t\t# determine if we are using a data generator or not\n",
        "\t\tuseGen = self.is_data_iter(trainData)\n",
        "\n",
        "\t\t# if we're using a generator and the steps per epoch is not\n",
        "\t\t# supplied, raise an error\n",
        "\t\tif useGen and stepsPerEpoch is None:\n",
        "\t\t\tmsg = \"Using generator without supplying stepsPerEpoch\"\n",
        "\t\t\traise Exception(msg)\n",
        "\n",
        "\t\t# if we're not using a generator then our entire dataset must\n",
        "\t\t# already be in memory\n",
        "\t\telif not useGen:\n",
        "\t\t\t# grab the number of samples in the training data and\n",
        "\t\t\t# then derive the number of steps per epoch\n",
        "\t\t\tnumSamples = len(trainData[0])\n",
        "\t\t\tstepsPerEpoch = np.ceil(numSamples / float(batchSize))\n",
        "\n",
        "\t\t# if no number of training epochs are supplied, compute the\n",
        "\t\t# training epochs based on a default sample size\n",
        "\t\tif epochs is None:\n",
        "\t\t\tepochs = int(np.ceil(sampleSize / float(stepsPerEpoch)))\n",
        "\n",
        "\t\t# compute the total number of batch updates that will take\n",
        "\t\t# place while we are attempting to find a good starting\n",
        "\t\t# learning rate\n",
        "\t\tnumBatchUpdates = epochs * stepsPerEpoch\n",
        "\n",
        "\t\t# derive the learning rate multiplier based on the ending\n",
        "\t\t# learning rate, starting learning rate, and total number of\n",
        "\t\t# batch updates\n",
        "\t\tself.lrMult = (endLR / startLR) ** (1.0 / numBatchUpdates)\n",
        "\n",
        "\t\t# create a temporary file path for the model weights and\n",
        "\t\t# then save the weights (so we can reset the weights when we\n",
        "\t\t# are done)\n",
        "\t\tself.weightsFile = tempfile.mkstemp()[1]\n",
        "\t\tself.model.save_weights(self.weightsFile)\n",
        "\n",
        "\t\t# grab the *original* learning rate (so we can reset it\n",
        "\t\t# later), and then set the *starting* learning rate\n",
        "\t\torigLR = K.get_value(self.model.optimizer.lr)\n",
        "\t\tK.set_value(self.model.optimizer.lr, startLR)\n",
        "\n",
        "\t\t# construct a callback that will be called at the end of each\n",
        "\t\t# batch, enabling us to increase our learning rate as training\n",
        "\t\t# progresses\n",
        "\t\tcallback = LambdaCallback(on_batch_end=lambda batch, logs:\n",
        "\t\t\tself.on_batch_end(batch, logs))\n",
        "\n",
        "\t\t# check to see if we are using a data iterator\n",
        "\t\tif useGen:\n",
        "\t\t\tself.model.fit_generator(\n",
        "\t\t\t\ttrainData,\n",
        "\t\t\t\tsteps_per_epoch=stepsPerEpoch,\n",
        "\t\t\t\tepochs=epochs,\n",
        "\t\t\t\tverbose=verbose,\n",
        "\t\t\t\tcallbacks=[callback])\n",
        "\n",
        "\t\t# otherwise, our entire training data is already in memory\n",
        "\t\telse:\n",
        "\t\t\t# train our model using Keras' fit method\n",
        "\t\t\tself.model.fit(\n",
        "\t\t\t\ttrainData[0], trainData[1],\n",
        "\t\t\t\tbatch_size=batchSize,\n",
        "\t\t\t\tepochs=epochs,\n",
        "\t\t\t\tcallbacks=[callback],\n",
        "\t\t\t\tverbose=verbose)\n",
        "\n",
        "\t\t# restore the original model weights and learning rate\n",
        "\t\tself.model.load_weights(self.weightsFile)\n",
        "\t\tK.set_value(self.model.optimizer.lr, origLR)\n",
        "\n",
        "\tdef plot_loss(self, skipBegin=10, skipEnd=1, title=\"\"):\n",
        "\t\t# grab the learning rate and losses values to plot\n",
        "\t\tlrs = self.lrs[skipBegin:-skipEnd]\n",
        "\t\tlosses = self.losses[skipBegin:-skipEnd]\n",
        "\n",
        "\t\t# plot the learning rate vs. loss\n",
        "\t\tplt.plot(lrs, losses)\n",
        "\t\tplt.xscale(\"log\")\n",
        "\t\tplt.xlabel(\"Learning Rate (Log Scale)\")\n",
        "\t\tplt.ylabel(\"Loss\")\n",
        "\n",
        "\t\t# if the title is not empty, add it to the plot\n",
        "\t\tif title != \"\":\n",
        "\t\t\tplt.title(title)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7tmzv3v5HgCh",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense, Conv2D, MaxPooling2D, Dropout\n",
        "from dataset import load_hoda\n",
        "import datetime"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9nOMK3cNHgCo",
        "colab": {}
      },
      "source": [
        "np.random.seed(123)  # for reproducibility\n",
        "\n",
        "# Load pre-shuffled HODA data into train and test sets\n",
        "x_train_original, y_train_original, x_test_original, y_test_original = load_hoda(\n",
        "                                                                        training_sample_size=3500, test_sample_size=400,size=28)\n",
        "\n",
        "# Preprocess input data\n",
        "''' 3.1: input data in numpy array format'''\n",
        "x_train = np.array(x_train_original)\n",
        "x_test = np.array(x_test_original)\n",
        "'''3.2 normalize our data values to the range [0, 1]'''\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# Reshape to original image shape (n x 784)  ==> (n x 28 x 28 x 1)\n",
        "x_train = x_train.reshape(-1,28,28,1)\n",
        "x_test = x_test.reshape(-1,28,28,1)\n",
        "\n",
        "\n",
        "# Preprocess class labels\n",
        "y_train = keras.utils.to_categorical(y_train_original, num_classes=10)\n",
        "y_test = keras.utils.to_categorical(y_test_original, num_classes=10)\n",
        "\n",
        "\n",
        "# test and validation set\n",
        "x_val = x_test[:200]\n",
        "x_test = x_test[200:]\n",
        "y_val = y_test[:200]\n",
        "y_test = y_test[200:]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IEprN75rHgCu",
        "colab": {}
      },
      "source": [
        "# Define model architecture\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu',\n",
        "                        input_shape=(28, 28, 1)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w_iRoK-_HgC5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e7bcee27-fda6-46b0-bf23-0b9402e7ac20"
      },
      "source": [
        "# Fit model on training data\n",
        "history = model.fit(x_train, y_train,\n",
        "          epochs=200, batch_size=256, validation_data = (x_val, y_val))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.0101 - accuracy: 0.9960 - val_loss: 0.0876 - val_accuracy: 0.9850\n",
            "Epoch 2/200\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0112 - accuracy: 0.9963 - val_loss: 0.1104 - val_accuracy: 0.9850\n",
            "Epoch 3/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0086 - accuracy: 0.9974 - val_loss: 0.1256 - val_accuracy: 0.9800\n",
            "Epoch 4/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0120 - accuracy: 0.9966 - val_loss: 0.1391 - val_accuracy: 0.9800\n",
            "Epoch 5/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0089 - accuracy: 0.9971 - val_loss: 0.1138 - val_accuracy: 0.9800\n",
            "Epoch 6/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0088 - accuracy: 0.9974 - val_loss: 0.1232 - val_accuracy: 0.9800\n",
            "Epoch 7/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0074 - accuracy: 0.9977 - val_loss: 0.1102 - val_accuracy: 0.9850\n",
            "Epoch 8/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0107 - accuracy: 0.9963 - val_loss: 0.1110 - val_accuracy: 0.9900\n",
            "Epoch 9/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0112 - accuracy: 0.9963 - val_loss: 0.1068 - val_accuracy: 0.9900\n",
            "Epoch 10/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0112 - accuracy: 0.9957 - val_loss: 0.0754 - val_accuracy: 0.9850\n",
            "Epoch 11/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0193 - accuracy: 0.9923 - val_loss: 0.1159 - val_accuracy: 0.9750\n",
            "Epoch 12/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0219 - accuracy: 0.9929 - val_loss: 0.0832 - val_accuracy: 0.9750\n",
            "Epoch 13/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0136 - accuracy: 0.9960 - val_loss: 0.0848 - val_accuracy: 0.9850\n",
            "Epoch 14/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0095 - accuracy: 0.9966 - val_loss: 0.0574 - val_accuracy: 0.9900\n",
            "Epoch 15/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0102 - accuracy: 0.9957 - val_loss: 0.0992 - val_accuracy: 0.9800\n",
            "Epoch 16/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0096 - accuracy: 0.9966 - val_loss: 0.0925 - val_accuracy: 0.9850\n",
            "Epoch 17/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0085 - accuracy: 0.9974 - val_loss: 0.1044 - val_accuracy: 0.9900\n",
            "Epoch 18/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0089 - accuracy: 0.9966 - val_loss: 0.0857 - val_accuracy: 0.9850\n",
            "Epoch 19/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0071 - accuracy: 0.9980 - val_loss: 0.0789 - val_accuracy: 0.9850\n",
            "Epoch 20/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0088 - accuracy: 0.9966 - val_loss: 0.1039 - val_accuracy: 0.9800\n",
            "Epoch 21/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0080 - accuracy: 0.9977 - val_loss: 0.0968 - val_accuracy: 0.9900\n",
            "Epoch 22/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0135 - accuracy: 0.9957 - val_loss: 0.1326 - val_accuracy: 0.9700\n",
            "Epoch 23/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0081 - accuracy: 0.9969 - val_loss: 0.0868 - val_accuracy: 0.9800\n",
            "Epoch 24/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0081 - accuracy: 0.9980 - val_loss: 0.0863 - val_accuracy: 0.9800\n",
            "Epoch 25/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0085 - accuracy: 0.9969 - val_loss: 0.0772 - val_accuracy: 0.9900\n",
            "Epoch 26/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0090 - accuracy: 0.9963 - val_loss: 0.0671 - val_accuracy: 0.9850\n",
            "Epoch 27/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0067 - accuracy: 0.9983 - val_loss: 0.0937 - val_accuracy: 0.9900\n",
            "Epoch 28/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0075 - accuracy: 0.9983 - val_loss: 0.0723 - val_accuracy: 0.9900\n",
            "Epoch 29/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0070 - accuracy: 0.9980 - val_loss: 0.0816 - val_accuracy: 0.9900\n",
            "Epoch 30/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0089 - accuracy: 0.9974 - val_loss: 0.1155 - val_accuracy: 0.9850\n",
            "Epoch 31/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0073 - accuracy: 0.9977 - val_loss: 0.1036 - val_accuracy: 0.9850\n",
            "Epoch 32/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0107 - accuracy: 0.9971 - val_loss: 0.0967 - val_accuracy: 0.9900\n",
            "Epoch 33/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0111 - accuracy: 0.9960 - val_loss: 0.0831 - val_accuracy: 0.9850\n",
            "Epoch 34/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0100 - accuracy: 0.9963 - val_loss: 0.1350 - val_accuracy: 0.9750\n",
            "Epoch 35/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0120 - accuracy: 0.9960 - val_loss: 0.0861 - val_accuracy: 0.9900\n",
            "Epoch 36/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0092 - accuracy: 0.9969 - val_loss: 0.0658 - val_accuracy: 0.9850\n",
            "Epoch 37/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0087 - accuracy: 0.9966 - val_loss: 0.0910 - val_accuracy: 0.9850\n",
            "Epoch 38/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0083 - accuracy: 0.9969 - val_loss: 0.0793 - val_accuracy: 0.9900\n",
            "Epoch 39/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0056 - accuracy: 0.9977 - val_loss: 0.0686 - val_accuracy: 0.9900\n",
            "Epoch 40/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0056 - accuracy: 0.9983 - val_loss: 0.0821 - val_accuracy: 0.9850\n",
            "Epoch 41/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0066 - accuracy: 0.9977 - val_loss: 0.0792 - val_accuracy: 0.9900\n",
            "Epoch 42/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0070 - accuracy: 0.9977 - val_loss: 0.0963 - val_accuracy: 0.9900\n",
            "Epoch 43/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0058 - accuracy: 0.9977 - val_loss: 0.1036 - val_accuracy: 0.9900\n",
            "Epoch 44/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0065 - accuracy: 0.9971 - val_loss: 0.0799 - val_accuracy: 0.9850\n",
            "Epoch 45/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0059 - accuracy: 0.9983 - val_loss: 0.0980 - val_accuracy: 0.9900\n",
            "Epoch 46/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0081 - accuracy: 0.9963 - val_loss: 0.1148 - val_accuracy: 0.9800\n",
            "Epoch 47/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0152 - accuracy: 0.9949 - val_loss: 0.0810 - val_accuracy: 0.9900\n",
            "Epoch 48/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0085 - accuracy: 0.9969 - val_loss: 0.0844 - val_accuracy: 0.9850\n",
            "Epoch 49/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0077 - accuracy: 0.9971 - val_loss: 0.0958 - val_accuracy: 0.9850\n",
            "Epoch 50/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0073 - accuracy: 0.9980 - val_loss: 0.0949 - val_accuracy: 0.9850\n",
            "Epoch 51/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0070 - accuracy: 0.9977 - val_loss: 0.0840 - val_accuracy: 0.9900\n",
            "Epoch 52/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0039 - accuracy: 0.9994 - val_loss: 0.0967 - val_accuracy: 0.9900\n",
            "Epoch 53/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0053 - accuracy: 0.9977 - val_loss: 0.0863 - val_accuracy: 0.9900\n",
            "Epoch 54/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0052 - accuracy: 0.9989 - val_loss: 0.0913 - val_accuracy: 0.9850\n",
            "Epoch 55/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0064 - accuracy: 0.9977 - val_loss: 0.1237 - val_accuracy: 0.9800\n",
            "Epoch 56/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0054 - accuracy: 0.9977 - val_loss: 0.1216 - val_accuracy: 0.9900\n",
            "Epoch 57/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0045 - accuracy: 0.9986 - val_loss: 0.1023 - val_accuracy: 0.9850\n",
            "Epoch 58/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0079 - accuracy: 0.9977 - val_loss: 0.1195 - val_accuracy: 0.9850\n",
            "Epoch 59/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0091 - accuracy: 0.9966 - val_loss: 0.0971 - val_accuracy: 0.9850\n",
            "Epoch 60/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0080 - accuracy: 0.9966 - val_loss: 0.0763 - val_accuracy: 0.9900\n",
            "Epoch 61/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0098 - accuracy: 0.9974 - val_loss: 0.0737 - val_accuracy: 0.9850\n",
            "Epoch 62/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0077 - accuracy: 0.9983 - val_loss: 0.0924 - val_accuracy: 0.9850\n",
            "Epoch 63/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0093 - accuracy: 0.9963 - val_loss: 0.0712 - val_accuracy: 0.9900\n",
            "Epoch 64/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0079 - accuracy: 0.9977 - val_loss: 0.0597 - val_accuracy: 0.9900\n",
            "Epoch 65/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0055 - accuracy: 0.9980 - val_loss: 0.0886 - val_accuracy: 0.9900\n",
            "Epoch 66/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0090 - accuracy: 0.9966 - val_loss: 0.0819 - val_accuracy: 0.9900\n",
            "Epoch 67/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0055 - accuracy: 0.9977 - val_loss: 0.0608 - val_accuracy: 0.9850\n",
            "Epoch 68/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0074 - accuracy: 0.9980 - val_loss: 0.0814 - val_accuracy: 0.9950\n",
            "Epoch 69/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0082 - accuracy: 0.9974 - val_loss: 0.0670 - val_accuracy: 0.9850\n",
            "Epoch 70/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.9971 - val_loss: 0.0986 - val_accuracy: 0.9800\n",
            "Epoch 71/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0067 - accuracy: 0.9971 - val_loss: 0.0947 - val_accuracy: 0.9900\n",
            "Epoch 72/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0080 - accuracy: 0.9974 - val_loss: 0.1022 - val_accuracy: 0.9750\n",
            "Epoch 73/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0062 - accuracy: 0.9980 - val_loss: 0.0555 - val_accuracy: 0.9900\n",
            "Epoch 74/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0052 - accuracy: 0.9986 - val_loss: 0.0929 - val_accuracy: 0.9850\n",
            "Epoch 75/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0053 - accuracy: 0.9983 - val_loss: 0.0833 - val_accuracy: 0.9850\n",
            "Epoch 76/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0037 - accuracy: 0.9994 - val_loss: 0.0751 - val_accuracy: 0.9850\n",
            "Epoch 77/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0066 - accuracy: 0.9969 - val_loss: 0.1013 - val_accuracy: 0.9850\n",
            "Epoch 78/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0067 - accuracy: 0.9971 - val_loss: 0.1083 - val_accuracy: 0.9850\n",
            "Epoch 79/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0067 - accuracy: 0.9969 - val_loss: 0.1145 - val_accuracy: 0.9850\n",
            "Epoch 80/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0054 - accuracy: 0.9983 - val_loss: 0.1056 - val_accuracy: 0.9850\n",
            "Epoch 81/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0075 - accuracy: 0.9977 - val_loss: 0.1334 - val_accuracy: 0.9800\n",
            "Epoch 82/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0076 - accuracy: 0.9969 - val_loss: 0.1267 - val_accuracy: 0.9900\n",
            "Epoch 83/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0088 - accuracy: 0.9974 - val_loss: 0.1294 - val_accuracy: 0.9800\n",
            "Epoch 84/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0062 - accuracy: 0.9980 - val_loss: 0.1115 - val_accuracy: 0.9750\n",
            "Epoch 85/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0084 - accuracy: 0.9969 - val_loss: 0.0836 - val_accuracy: 0.9900\n",
            "Epoch 86/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.0967 - val_accuracy: 0.9900\n",
            "Epoch 87/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0044 - accuracy: 0.9980 - val_loss: 0.0918 - val_accuracy: 0.9900\n",
            "Epoch 88/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0049 - accuracy: 0.9989 - val_loss: 0.0969 - val_accuracy: 0.9900\n",
            "Epoch 89/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0126 - accuracy: 0.9954 - val_loss: 0.1421 - val_accuracy: 0.9850\n",
            "Epoch 90/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0112 - accuracy: 0.9966 - val_loss: 0.0535 - val_accuracy: 0.9900\n",
            "Epoch 91/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0151 - accuracy: 0.9957 - val_loss: 0.0637 - val_accuracy: 0.9850\n",
            "Epoch 92/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0145 - accuracy: 0.9946 - val_loss: 0.1742 - val_accuracy: 0.9750\n",
            "Epoch 93/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0079 - accuracy: 0.9971 - val_loss: 0.0668 - val_accuracy: 0.9900\n",
            "Epoch 94/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0064 - accuracy: 0.9983 - val_loss: 0.0790 - val_accuracy: 0.9900\n",
            "Epoch 95/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0063 - accuracy: 0.9971 - val_loss: 0.0970 - val_accuracy: 0.9850\n",
            "Epoch 96/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0061 - accuracy: 0.9980 - val_loss: 0.1049 - val_accuracy: 0.9850\n",
            "Epoch 97/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0056 - accuracy: 0.9977 - val_loss: 0.1053 - val_accuracy: 0.9850\n",
            "Epoch 98/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0039 - accuracy: 0.9986 - val_loss: 0.1354 - val_accuracy: 0.9850\n",
            "Epoch 99/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0074 - accuracy: 0.9969 - val_loss: 0.0778 - val_accuracy: 0.9850\n",
            "Epoch 100/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0650 - val_accuracy: 0.9900\n",
            "Epoch 101/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0047 - accuracy: 0.9980 - val_loss: 0.0965 - val_accuracy: 0.9900\n",
            "Epoch 102/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0033 - accuracy: 0.9986 - val_loss: 0.1011 - val_accuracy: 0.9850\n",
            "Epoch 103/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0036 - accuracy: 0.9986 - val_loss: 0.0928 - val_accuracy: 0.9800\n",
            "Epoch 104/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0042 - accuracy: 0.9989 - val_loss: 0.0881 - val_accuracy: 0.9900\n",
            "Epoch 105/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0032 - accuracy: 0.9983 - val_loss: 0.1198 - val_accuracy: 0.9850\n",
            "Epoch 106/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0037 - accuracy: 0.9983 - val_loss: 0.0946 - val_accuracy: 0.9850\n",
            "Epoch 107/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0037 - accuracy: 0.9986 - val_loss: 0.1250 - val_accuracy: 0.9850\n",
            "Epoch 108/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0059 - accuracy: 0.9980 - val_loss: 0.1017 - val_accuracy: 0.9800\n",
            "Epoch 109/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0062 - accuracy: 0.9977 - val_loss: 0.0960 - val_accuracy: 0.9900\n",
            "Epoch 110/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0051 - accuracy: 0.9986 - val_loss: 0.0744 - val_accuracy: 0.9900\n",
            "Epoch 111/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0039 - accuracy: 0.9986 - val_loss: 0.0921 - val_accuracy: 0.9850\n",
            "Epoch 112/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0060 - accuracy: 0.9977 - val_loss: 0.0809 - val_accuracy: 0.9900\n",
            "Epoch 113/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0034 - accuracy: 0.9986 - val_loss: 0.0867 - val_accuracy: 0.9850\n",
            "Epoch 114/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0057 - accuracy: 0.9980 - val_loss: 0.0736 - val_accuracy: 0.9900\n",
            "Epoch 115/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.9974 - val_loss: 0.0659 - val_accuracy: 0.9900\n",
            "Epoch 116/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0057 - accuracy: 0.9971 - val_loss: 0.0377 - val_accuracy: 0.9900\n",
            "Epoch 117/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0066 - accuracy: 0.9977 - val_loss: 0.0575 - val_accuracy: 0.9900\n",
            "Epoch 118/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0042 - accuracy: 0.9994 - val_loss: 0.0599 - val_accuracy: 0.9850\n",
            "Epoch 119/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0062 - accuracy: 0.9971 - val_loss: 0.0583 - val_accuracy: 0.9900\n",
            "Epoch 120/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0042 - accuracy: 0.9983 - val_loss: 0.0474 - val_accuracy: 0.9900\n",
            "Epoch 121/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0024 - accuracy: 0.9997 - val_loss: 0.0477 - val_accuracy: 0.9900\n",
            "Epoch 122/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0045 - accuracy: 0.9980 - val_loss: 0.0720 - val_accuracy: 0.9900\n",
            "Epoch 123/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0042 - accuracy: 0.9986 - val_loss: 0.0876 - val_accuracy: 0.9850\n",
            "Epoch 124/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0024 - accuracy: 0.9997 - val_loss: 0.0905 - val_accuracy: 0.9900\n",
            "Epoch 125/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0044 - accuracy: 0.9986 - val_loss: 0.0971 - val_accuracy: 0.9900\n",
            "Epoch 126/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0031 - accuracy: 0.9989 - val_loss: 0.0712 - val_accuracy: 0.9900\n",
            "Epoch 127/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0032 - accuracy: 0.9989 - val_loss: 0.0691 - val_accuracy: 0.9900\n",
            "Epoch 128/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0976 - val_accuracy: 0.9800\n",
            "Epoch 129/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0077 - accuracy: 0.9971 - val_loss: 0.0558 - val_accuracy: 0.9900\n",
            "Epoch 130/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0064 - accuracy: 0.9974 - val_loss: 0.0414 - val_accuracy: 0.9900\n",
            "Epoch 131/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0071 - accuracy: 0.9974 - val_loss: 0.0776 - val_accuracy: 0.9850\n",
            "Epoch 132/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0122 - accuracy: 0.9966 - val_loss: 0.0449 - val_accuracy: 0.9850\n",
            "Epoch 133/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0039 - accuracy: 0.9989 - val_loss: 0.0608 - val_accuracy: 0.9900\n",
            "Epoch 134/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0674 - val_accuracy: 0.9850\n",
            "Epoch 135/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0045 - accuracy: 0.9980 - val_loss: 0.0638 - val_accuracy: 0.9850\n",
            "Epoch 136/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.0624 - val_accuracy: 0.9850\n",
            "Epoch 137/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0034 - accuracy: 0.9986 - val_loss: 0.0649 - val_accuracy: 0.9850\n",
            "Epoch 138/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.0520 - val_accuracy: 0.9900\n",
            "Epoch 139/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0050 - accuracy: 0.9977 - val_loss: 0.1058 - val_accuracy: 0.9800\n",
            "Epoch 140/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0056 - accuracy: 0.9983 - val_loss: 0.1152 - val_accuracy: 0.9800\n",
            "Epoch 141/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0083 - accuracy: 0.9969 - val_loss: 0.0547 - val_accuracy: 0.9850\n",
            "Epoch 142/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0066 - accuracy: 0.9977 - val_loss: 0.1266 - val_accuracy: 0.9800\n",
            "Epoch 143/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0070 - accuracy: 0.9969 - val_loss: 0.1099 - val_accuracy: 0.9850\n",
            "Epoch 144/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0044 - accuracy: 0.9986 - val_loss: 0.1131 - val_accuracy: 0.9750\n",
            "Epoch 145/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0059 - accuracy: 0.9989 - val_loss: 0.0619 - val_accuracy: 0.9850\n",
            "Epoch 146/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0070 - accuracy: 0.9974 - val_loss: 0.1171 - val_accuracy: 0.9850\n",
            "Epoch 147/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0080 - accuracy: 0.9983 - val_loss: 0.1304 - val_accuracy: 0.9800\n",
            "Epoch 148/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0077 - accuracy: 0.9977 - val_loss: 0.0676 - val_accuracy: 0.9850\n",
            "Epoch 149/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0063 - accuracy: 0.9980 - val_loss: 0.0882 - val_accuracy: 0.9850\n",
            "Epoch 150/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0065 - accuracy: 0.9983 - val_loss: 0.0877 - val_accuracy: 0.9850\n",
            "Epoch 151/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0050 - accuracy: 0.9983 - val_loss: 0.0825 - val_accuracy: 0.9800\n",
            "Epoch 152/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0028 - accuracy: 0.9991 - val_loss: 0.0936 - val_accuracy: 0.9800\n",
            "Epoch 153/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0040 - accuracy: 0.9989 - val_loss: 0.0889 - val_accuracy: 0.9800\n",
            "Epoch 154/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.1180 - val_accuracy: 0.9800\n",
            "Epoch 155/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0036 - accuracy: 0.9986 - val_loss: 0.1279 - val_accuracy: 0.9750\n",
            "Epoch 156/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0021 - accuracy: 0.9991 - val_loss: 0.1113 - val_accuracy: 0.9800\n",
            "Epoch 157/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0049 - accuracy: 0.9980 - val_loss: 0.1376 - val_accuracy: 0.9800\n",
            "Epoch 158/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0061 - accuracy: 0.9983 - val_loss: 0.1448 - val_accuracy: 0.9850\n",
            "Epoch 159/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0027 - accuracy: 0.9989 - val_loss: 0.1206 - val_accuracy: 0.9800\n",
            "Epoch 160/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0028 - accuracy: 0.9986 - val_loss: 0.0973 - val_accuracy: 0.9850\n",
            "Epoch 161/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.0956 - val_accuracy: 0.9850\n",
            "Epoch 162/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.9974 - val_loss: 0.1957 - val_accuracy: 0.9750\n",
            "Epoch 163/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0113 - accuracy: 0.9954 - val_loss: 0.0825 - val_accuracy: 0.9850\n",
            "Epoch 164/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0056 - accuracy: 0.9974 - val_loss: 0.1222 - val_accuracy: 0.9850\n",
            "Epoch 165/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0070 - accuracy: 0.9969 - val_loss: 0.0762 - val_accuracy: 0.9900\n",
            "Epoch 166/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0065 - accuracy: 0.9977 - val_loss: 0.0957 - val_accuracy: 0.9900\n",
            "Epoch 167/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0040 - accuracy: 0.9989 - val_loss: 0.1039 - val_accuracy: 0.9900\n",
            "Epoch 168/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0038 - accuracy: 0.9986 - val_loss: 0.0921 - val_accuracy: 0.9900\n",
            "Epoch 169/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.1020 - val_accuracy: 0.9850\n",
            "Epoch 170/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0015 - accuracy: 0.9994 - val_loss: 0.0919 - val_accuracy: 0.9900\n",
            "Epoch 171/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.0902 - val_accuracy: 0.9850\n",
            "Epoch 172/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0042 - accuracy: 0.9980 - val_loss: 0.0941 - val_accuracy: 0.9850\n",
            "Epoch 173/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.0968 - val_accuracy: 0.9850\n",
            "Epoch 174/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0024 - accuracy: 0.9991 - val_loss: 0.0999 - val_accuracy: 0.9900\n",
            "Epoch 175/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0035 - accuracy: 0.9989 - val_loss: 0.0969 - val_accuracy: 0.9850\n",
            "Epoch 176/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0025 - accuracy: 0.9991 - val_loss: 0.1604 - val_accuracy: 0.9850\n",
            "Epoch 177/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0029 - accuracy: 0.9986 - val_loss: 0.1227 - val_accuracy: 0.9900\n",
            "Epoch 178/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0043 - accuracy: 0.9983 - val_loss: 0.1152 - val_accuracy: 0.9850\n",
            "Epoch 179/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0044 - accuracy: 0.9989 - val_loss: 0.1157 - val_accuracy: 0.9800\n",
            "Epoch 180/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0061 - accuracy: 0.9983 - val_loss: 0.0942 - val_accuracy: 0.9850\n",
            "Epoch 181/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0052 - accuracy: 0.9977 - val_loss: 0.1339 - val_accuracy: 0.9850\n",
            "Epoch 182/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0037 - accuracy: 0.9991 - val_loss: 0.1650 - val_accuracy: 0.9800\n",
            "Epoch 183/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0040 - accuracy: 0.9991 - val_loss: 0.1382 - val_accuracy: 0.9850\n",
            "Epoch 184/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0039 - accuracy: 0.9991 - val_loss: 0.1716 - val_accuracy: 0.9800\n",
            "Epoch 185/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0031 - accuracy: 0.9989 - val_loss: 0.1493 - val_accuracy: 0.9850\n",
            "Epoch 186/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0123 - accuracy: 0.9969 - val_loss: 0.2286 - val_accuracy: 0.9800\n",
            "Epoch 187/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0094 - accuracy: 0.9960 - val_loss: 0.1243 - val_accuracy: 0.9800\n",
            "Epoch 188/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0093 - accuracy: 0.9969 - val_loss: 0.1489 - val_accuracy: 0.9850\n",
            "Epoch 189/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0062 - accuracy: 0.9983 - val_loss: 0.2177 - val_accuracy: 0.9700\n",
            "Epoch 190/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0048 - accuracy: 0.9989 - val_loss: 0.1697 - val_accuracy: 0.9800\n",
            "Epoch 191/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0039 - accuracy: 0.9991 - val_loss: 0.1402 - val_accuracy: 0.9800\n",
            "Epoch 192/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0055 - accuracy: 0.9983 - val_loss: 0.1842 - val_accuracy: 0.9800\n",
            "Epoch 193/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0063 - accuracy: 0.9980 - val_loss: 0.1358 - val_accuracy: 0.9850\n",
            "Epoch 194/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.9980 - val_loss: 0.1831 - val_accuracy: 0.9850\n",
            "Epoch 195/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0049 - accuracy: 0.9986 - val_loss: 0.1721 - val_accuracy: 0.9850\n",
            "Epoch 196/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0043 - accuracy: 0.9986 - val_loss: 0.1719 - val_accuracy: 0.9850\n",
            "Epoch 197/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0043 - accuracy: 0.9980 - val_loss: 0.1741 - val_accuracy: 0.9850\n",
            "Epoch 198/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0067 - accuracy: 0.9977 - val_loss: 0.1650 - val_accuracy: 0.9850\n",
            "Epoch 199/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0040 - accuracy: 0.9989 - val_loss: 0.1693 - val_accuracy: 0.9800\n",
            "Epoch 200/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0048 - accuracy: 0.9983 - val_loss: 0.1708 - val_accuracy: 0.9750\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGA66LeXMwK2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "771be103-81a9-4251-d114-461bb95dc519"
      },
      "source": [
        "# initialize the learning rate finder and then train with learning\n",
        "lrf = LearningRateFinder(model)\n",
        "lrf.find((x_train, y_train),1e-10, 1e+1)\n",
        "# plot the loss for the various learning rates and save the\n",
        "# resulting plot to disk\n",
        "lrf.plot_loss()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/19\n",
            "110/110 [==============================] - 0s 4ms/step - loss: 0.0041 - accuracy: 0.9986\n",
            "Epoch 2/19\n",
            "110/110 [==============================] - 0s 4ms/step - loss: 0.0043 - accuracy: 0.9980\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}