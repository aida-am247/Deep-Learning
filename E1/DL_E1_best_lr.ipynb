{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_E1_best_lr.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOLIoh/sFmcQo8RmaziAgYo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aida-am247/Deep-Learning/blob/master/E1/DL_E1_best_lr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L2wEKO-zHgCX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "outputId": "88e052d2-0387-4099-81ae-c506bb6617f1"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/Alireza-Akhavan/SRU-deeplearning-workshop/master/dataset.py\n",
        "!mkdir dataset\n",
        "!wget https://github.com/Alireza-Akhavan/SRU-deeplearning-workshop/raw/master/dataset/Data_hoda_full.mat -P dataset"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-22 15:09:33--  https://raw.githubusercontent.com/Alireza-Akhavan/SRU-deeplearning-workshop/master/dataset.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 929 [text/plain]\n",
            "Saving to: ‘dataset.py’\n",
            "\n",
            "\rdataset.py            0%[                    ]       0  --.-KB/s               \rdataset.py          100%[===================>]     929  --.-KB/s    in 0s      \n",
            "\n",
            "2020-09-22 15:09:33 (52.0 MB/s) - ‘dataset.py’ saved [929/929]\n",
            "\n",
            "--2020-09-22 15:09:34--  https://github.com/Alireza-Akhavan/SRU-deeplearning-workshop/raw/master/dataset/Data_hoda_full.mat\n",
            "Resolving github.com (github.com)... 140.82.118.4\n",
            "Connecting to github.com (github.com)|140.82.118.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Alireza-Akhavan/SRU-deeplearning-workshop/master/dataset/Data_hoda_full.mat [following]\n",
            "--2020-09-22 15:09:34--  https://raw.githubusercontent.com/Alireza-Akhavan/SRU-deeplearning-workshop/master/dataset/Data_hoda_full.mat\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3989009 (3.8M) [application/octet-stream]\n",
            "Saving to: ‘dataset/Data_hoda_full.mat’\n",
            "\n",
            "Data_hoda_full.mat  100%[===================>]   3.80M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-09-22 15:09:35 (34.7 MB/s) - ‘dataset/Data_hoda_full.mat’ saved [3989009/3989009]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OfJsXzgOvxF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the necessary packages\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tempfile\n",
        "\n",
        "class LearningRateFinder:\n",
        "\tdef __init__(self, model, stopFactor=4, beta=0.98):\n",
        "\t\t# store the model, stop factor, and beta value (for computing\n",
        "\t\t# a smoothed, average loss)\n",
        "\t\tself.model = model\n",
        "\t\tself.stopFactor = stopFactor\n",
        "\t\tself.beta = beta\n",
        "\n",
        "\t\t# initialize our list of learning rates and losses,\n",
        "\t\t# respectively\n",
        "\t\tself.lrs = []\n",
        "\t\tself.losses = []\n",
        "\n",
        "\t\t# initialize our learning rate multiplier, average loss, best\n",
        "\t\t# loss found thus far, current batch number, and weights file\n",
        "\t\tself.lrMult = 1\n",
        "\t\tself.avgLoss = 0\n",
        "\t\tself.bestLoss = 1e9\n",
        "\t\tself.batchNum = 0\n",
        "\t\tself.weightsFile = None\n",
        "\n",
        "\tdef reset(self):\n",
        "\t\t# re-initialize all variables from our constructor\n",
        "\t\tself.lrs = []\n",
        "\t\tself.losses = []\n",
        "\t\tself.lrMult = 1\n",
        "\t\tself.avgLoss = 0\n",
        "\t\tself.bestLoss = 1e9\n",
        "\t\tself.batchNum = 0\n",
        "\t\tself.weightsFile = None\n",
        "\n",
        "\tdef is_data_iter(self, data):\n",
        "\t\t# define the set of class types we will check for\n",
        "\t\titerClasses = [\"NumpyArrayIterator\", \"DirectoryIterator\",\n",
        "\t\t\t \"Iterator\", \"Sequence\"]\n",
        "\n",
        "\t\t# return whether our data is an iterator\n",
        "\t\treturn data.__class__.__name__ in iterClasses\n",
        "\n",
        "\tdef on_batch_end(self, batch, logs):\n",
        "\t\t# grab the current learning rate and add log it to the list of\n",
        "\t\t# learning rates that we've tried\n",
        "\t\tlr = K.get_value(self.model.optimizer.lr)\n",
        "\t\tself.lrs.append(lr)\n",
        "\n",
        "\t\t# grab the loss at the end of this batch, increment the total\n",
        "\t\t# number of batches processed, compute the average average\n",
        "\t\t# loss, smooth it, and update the losses list with the\n",
        "\t\t# smoothed value\n",
        "\t\tl = logs[\"loss\"]\n",
        "\t\tself.batchNum += 1\n",
        "\t\tself.avgLoss = (self.beta * self.avgLoss) + ((1 - self.beta) * l)\n",
        "\t\tsmooth = self.avgLoss / (1 - (self.beta ** self.batchNum))\n",
        "\t\tself.losses.append(smooth)\n",
        "\n",
        "\t\t# compute the maximum loss stopping factor value\n",
        "\t\tstopLoss = self.stopFactor * self.bestLoss\n",
        "\n",
        "\t\t# check to see whether the loss has grown too large\n",
        "\t\tif self.batchNum > 1 and smooth > stopLoss:\n",
        "\t\t\t# stop returning and return from the method\n",
        "\t\t\tself.model.stop_training = True\n",
        "\t\t\treturn\n",
        "\n",
        "\t\t# check to see if the best loss should be updated\n",
        "\t\tif self.batchNum == 1 or smooth < self.bestLoss:\n",
        "\t\t\tself.bestLoss = smooth\n",
        "\n",
        "\t\t# increase the learning rate\n",
        "\t\tlr *= self.lrMult\n",
        "\t\tK.set_value(self.model.optimizer.lr, lr)\n",
        "\n",
        "\tdef find(self, trainData, startLR, endLR, epochs=None,\n",
        "\t\tstepsPerEpoch=None, batchSize=32, sampleSize=2048,\n",
        "\t\tverbose=1):\n",
        "\t\t# reset our class-specific variables\n",
        "\t\tself.reset()\n",
        "\n",
        "\t\t# determine if we are using a data generator or not\n",
        "\t\tuseGen = self.is_data_iter(trainData)\n",
        "\n",
        "\t\t# if we're using a generator and the steps per epoch is not\n",
        "\t\t# supplied, raise an error\n",
        "\t\tif useGen and stepsPerEpoch is None:\n",
        "\t\t\tmsg = \"Using generator without supplying stepsPerEpoch\"\n",
        "\t\t\traise Exception(msg)\n",
        "\n",
        "\t\t# if we're not using a generator then our entire dataset must\n",
        "\t\t# already be in memory\n",
        "\t\telif not useGen:\n",
        "\t\t\t# grab the number of samples in the training data and\n",
        "\t\t\t# then derive the number of steps per epoch\n",
        "\t\t\tnumSamples = len(trainData[0])\n",
        "\t\t\tstepsPerEpoch = np.ceil(numSamples / float(batchSize))\n",
        "\n",
        "\t\t# if no number of training epochs are supplied, compute the\n",
        "\t\t# training epochs based on a default sample size\n",
        "\t\tif epochs is None:\n",
        "\t\t\tepochs = int(np.ceil(sampleSize / float(stepsPerEpoch)))\n",
        "\n",
        "\t\t# compute the total number of batch updates that will take\n",
        "\t\t# place while we are attempting to find a good starting\n",
        "\t\t# learning rate\n",
        "\t\tnumBatchUpdates = epochs * stepsPerEpoch\n",
        "\n",
        "\t\t# derive the learning rate multiplier based on the ending\n",
        "\t\t# learning rate, starting learning rate, and total number of\n",
        "\t\t# batch updates\n",
        "\t\tself.lrMult = (endLR / startLR) ** (1.0 / numBatchUpdates)\n",
        "\n",
        "\t\t# create a temporary file path for the model weights and\n",
        "\t\t# then save the weights (so we can reset the weights when we\n",
        "\t\t# are done)\n",
        "\t\tself.weightsFile = tempfile.mkstemp()[1]\n",
        "\t\tself.model.save_weights(self.weightsFile)\n",
        "\n",
        "\t\t# grab the *original* learning rate (so we can reset it\n",
        "\t\t# later), and then set the *starting* learning rate\n",
        "\t\torigLR = K.get_value(self.model.optimizer.lr)\n",
        "\t\tK.set_value(self.model.optimizer.lr, startLR)\n",
        "\n",
        "\t\t# construct a callback that will be called at the end of each\n",
        "\t\t# batch, enabling us to increase our learning rate as training\n",
        "\t\t# progresses\n",
        "\t\tcallback = LambdaCallback(on_batch_end=lambda batch, logs:\n",
        "\t\t\tself.on_batch_end(batch, logs))\n",
        "\n",
        "\t\t# check to see if we are using a data iterator\n",
        "\t\tif useGen:\n",
        "\t\t\tself.model.fit_generator(\n",
        "\t\t\t\ttrainData,\n",
        "\t\t\t\tsteps_per_epoch=stepsPerEpoch,\n",
        "\t\t\t\tepochs=epochs,\n",
        "\t\t\t\tverbose=verbose,\n",
        "\t\t\t\tcallbacks=[callback])\n",
        "\n",
        "\t\t# otherwise, our entire training data is already in memory\n",
        "\t\telse:\n",
        "\t\t\t# train our model using Keras' fit method\n",
        "\t\t\tself.model.fit(\n",
        "\t\t\t\ttrainData[0], trainData[1],\n",
        "\t\t\t\tbatch_size=batchSize,\n",
        "\t\t\t\tepochs=epochs,\n",
        "\t\t\t\tcallbacks=[callback],\n",
        "\t\t\t\tverbose=verbose)\n",
        "\n",
        "\t\t# restore the original model weights and learning rate\n",
        "\t\tself.model.load_weights(self.weightsFile)\n",
        "\t\tK.set_value(self.model.optimizer.lr, origLR)\n",
        "\n",
        "\tdef plot_loss(self, skipBegin=10, skipEnd=1, title=\"\"):\n",
        "\t\t# grab the learning rate and losses values to plot\n",
        "\t\tlrs = self.lrs[skipBegin:-skipEnd]\n",
        "\t\tlosses = self.losses[skipBegin:-skipEnd]\n",
        "\n",
        "\t\t# plot the learning rate vs. loss\n",
        "\t\tplt.plot(lrs, losses)\n",
        "\t\tplt.xscale(\"log\")\n",
        "\t\tplt.xlabel(\"Learning Rate (Log Scale)\")\n",
        "\t\tplt.ylabel(\"Loss\")\n",
        "\n",
        "\t\t# if the title is not empty, add it to the plot\n",
        "\t\tif title != \"\":\n",
        "\t\t\tplt.title(title)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7tmzv3v5HgCh",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense, Conv2D, MaxPooling2D, Dropout\n",
        "from dataset import load_hoda\n",
        "import datetime"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9nOMK3cNHgCo",
        "colab": {}
      },
      "source": [
        "np.random.seed(123)  # for reproducibility\n",
        "\n",
        "# Load pre-shuffled HODA data into train and test sets\n",
        "x_train_original, y_train_original, x_test_original, y_test_original = load_hoda(\n",
        "                                                                        training_sample_size=3500, test_sample_size=400,size=28)\n",
        "\n",
        "# Preprocess input data\n",
        "''' 3.1: input data in numpy array format'''\n",
        "x_train = np.array(x_train_original)\n",
        "x_test = np.array(x_test_original)\n",
        "'''3.2 normalize our data values to the range [0, 1]'''\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# Reshape to original image shape (n x 784)  ==> (n x 28 x 28 x 1)\n",
        "x_train = x_train.reshape(-1,28,28,1)\n",
        "x_test = x_test.reshape(-1,28,28,1)\n",
        "\n",
        "\n",
        "# Preprocess class labels\n",
        "y_train = keras.utils.to_categorical(y_train_original, num_classes=10)\n",
        "y_test = keras.utils.to_categorical(y_test_original, num_classes=10)\n",
        "\n",
        "\n",
        "# test and validation set\n",
        "x_val = x_test[:200]\n",
        "x_test = x_test[200:]\n",
        "y_val = y_test[:200]\n",
        "y_test = y_test[200:]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IEprN75rHgCu",
        "colab": {}
      },
      "source": [
        "# Define model architecture\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu',\n",
        "                        input_shape=(28, 28, 1)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w_iRoK-_HgC5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e7bcee27-fda6-46b0-bf23-0b9402e7ac20"
      },
      "source": [
        "# Fit model on training data\n",
        "history = model.fit(x_train, y_train,\n",
        "          epochs=200, batch_size=256, validation_data = (x_val, y_val))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.0101 - accuracy: 0.9960 - val_loss: 0.0876 - val_accuracy: 0.9850\n",
            "Epoch 2/200\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0112 - accuracy: 0.9963 - val_loss: 0.1104 - val_accuracy: 0.9850\n",
            "Epoch 3/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0086 - accuracy: 0.9974 - val_loss: 0.1256 - val_accuracy: 0.9800\n",
            "Epoch 4/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0120 - accuracy: 0.9966 - val_loss: 0.1391 - val_accuracy: 0.9800\n",
            "Epoch 5/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0089 - accuracy: 0.9971 - val_loss: 0.1138 - val_accuracy: 0.9800\n",
            "Epoch 6/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0088 - accuracy: 0.9974 - val_loss: 0.1232 - val_accuracy: 0.9800\n",
            "Epoch 7/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0074 - accuracy: 0.9977 - val_loss: 0.1102 - val_accuracy: 0.9850\n",
            "Epoch 8/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0107 - accuracy: 0.9963 - val_loss: 0.1110 - val_accuracy: 0.9900\n",
            "Epoch 9/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0112 - accuracy: 0.9963 - val_loss: 0.1068 - val_accuracy: 0.9900\n",
            "Epoch 10/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0112 - accuracy: 0.9957 - val_loss: 0.0754 - val_accuracy: 0.9850\n",
            "Epoch 11/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0193 - accuracy: 0.9923 - val_loss: 0.1159 - val_accuracy: 0.9750\n",
            "Epoch 12/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0219 - accuracy: 0.9929 - val_loss: 0.0832 - val_accuracy: 0.9750\n",
            "Epoch 13/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0136 - accuracy: 0.9960 - val_loss: 0.0848 - val_accuracy: 0.9850\n",
            "Epoch 14/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0095 - accuracy: 0.9966 - val_loss: 0.0574 - val_accuracy: 0.9900\n",
            "Epoch 15/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0102 - accuracy: 0.9957 - val_loss: 0.0992 - val_accuracy: 0.9800\n",
            "Epoch 16/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0096 - accuracy: 0.9966 - val_loss: 0.0925 - val_accuracy: 0.9850\n",
            "Epoch 17/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0085 - accuracy: 0.9974 - val_loss: 0.1044 - val_accuracy: 0.9900\n",
            "Epoch 18/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0089 - accuracy: 0.9966 - val_loss: 0.0857 - val_accuracy: 0.9850\n",
            "Epoch 19/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0071 - accuracy: 0.9980 - val_loss: 0.0789 - val_accuracy: 0.9850\n",
            "Epoch 20/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0088 - accuracy: 0.9966 - val_loss: 0.1039 - val_accuracy: 0.9800\n",
            "Epoch 21/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0080 - accuracy: 0.9977 - val_loss: 0.0968 - val_accuracy: 0.9900\n",
            "Epoch 22/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0135 - accuracy: 0.9957 - val_loss: 0.1326 - val_accuracy: 0.9700\n",
            "Epoch 23/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0081 - accuracy: 0.9969 - val_loss: 0.0868 - val_accuracy: 0.9800\n",
            "Epoch 24/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0081 - accuracy: 0.9980 - val_loss: 0.0863 - val_accuracy: 0.9800\n",
            "Epoch 25/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0085 - accuracy: 0.9969 - val_loss: 0.0772 - val_accuracy: 0.9900\n",
            "Epoch 26/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0090 - accuracy: 0.9963 - val_loss: 0.0671 - val_accuracy: 0.9850\n",
            "Epoch 27/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0067 - accuracy: 0.9983 - val_loss: 0.0937 - val_accuracy: 0.9900\n",
            "Epoch 28/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0075 - accuracy: 0.9983 - val_loss: 0.0723 - val_accuracy: 0.9900\n",
            "Epoch 29/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0070 - accuracy: 0.9980 - val_loss: 0.0816 - val_accuracy: 0.9900\n",
            "Epoch 30/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0089 - accuracy: 0.9974 - val_loss: 0.1155 - val_accuracy: 0.9850\n",
            "Epoch 31/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0073 - accuracy: 0.9977 - val_loss: 0.1036 - val_accuracy: 0.9850\n",
            "Epoch 32/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0107 - accuracy: 0.9971 - val_loss: 0.0967 - val_accuracy: 0.9900\n",
            "Epoch 33/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0111 - accuracy: 0.9960 - val_loss: 0.0831 - val_accuracy: 0.9850\n",
            "Epoch 34/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0100 - accuracy: 0.9963 - val_loss: 0.1350 - val_accuracy: 0.9750\n",
            "Epoch 35/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0120 - accuracy: 0.9960 - val_loss: 0.0861 - val_accuracy: 0.9900\n",
            "Epoch 36/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0092 - accuracy: 0.9969 - val_loss: 0.0658 - val_accuracy: 0.9850\n",
            "Epoch 37/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0087 - accuracy: 0.9966 - val_loss: 0.0910 - val_accuracy: 0.9850\n",
            "Epoch 38/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0083 - accuracy: 0.9969 - val_loss: 0.0793 - val_accuracy: 0.9900\n",
            "Epoch 39/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0056 - accuracy: 0.9977 - val_loss: 0.0686 - val_accuracy: 0.9900\n",
            "Epoch 40/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0056 - accuracy: 0.9983 - val_loss: 0.0821 - val_accuracy: 0.9850\n",
            "Epoch 41/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0066 - accuracy: 0.9977 - val_loss: 0.0792 - val_accuracy: 0.9900\n",
            "Epoch 42/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0070 - accuracy: 0.9977 - val_loss: 0.0963 - val_accuracy: 0.9900\n",
            "Epoch 43/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0058 - accuracy: 0.9977 - val_loss: 0.1036 - val_accuracy: 0.9900\n",
            "Epoch 44/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0065 - accuracy: 0.9971 - val_loss: 0.0799 - val_accuracy: 0.9850\n",
            "Epoch 45/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0059 - accuracy: 0.9983 - val_loss: 0.0980 - val_accuracy: 0.9900\n",
            "Epoch 46/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0081 - accuracy: 0.9963 - val_loss: 0.1148 - val_accuracy: 0.9800\n",
            "Epoch 47/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0152 - accuracy: 0.9949 - val_loss: 0.0810 - val_accuracy: 0.9900\n",
            "Epoch 48/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0085 - accuracy: 0.9969 - val_loss: 0.0844 - val_accuracy: 0.9850\n",
            "Epoch 49/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0077 - accuracy: 0.9971 - val_loss: 0.0958 - val_accuracy: 0.9850\n",
            "Epoch 50/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0073 - accuracy: 0.9980 - val_loss: 0.0949 - val_accuracy: 0.9850\n",
            "Epoch 51/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0070 - accuracy: 0.9977 - val_loss: 0.0840 - val_accuracy: 0.9900\n",
            "Epoch 52/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0039 - accuracy: 0.9994 - val_loss: 0.0967 - val_accuracy: 0.9900\n",
            "Epoch 53/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0053 - accuracy: 0.9977 - val_loss: 0.0863 - val_accuracy: 0.9900\n",
            "Epoch 54/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0052 - accuracy: 0.9989 - val_loss: 0.0913 - val_accuracy: 0.9850\n",
            "Epoch 55/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0064 - accuracy: 0.9977 - val_loss: 0.1237 - val_accuracy: 0.9800\n",
            "Epoch 56/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0054 - accuracy: 0.9977 - val_loss: 0.1216 - val_accuracy: 0.9900\n",
            "Epoch 57/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0045 - accuracy: 0.9986 - val_loss: 0.1023 - val_accuracy: 0.9850\n",
            "Epoch 58/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0079 - accuracy: 0.9977 - val_loss: 0.1195 - val_accuracy: 0.9850\n",
            "Epoch 59/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0091 - accuracy: 0.9966 - val_loss: 0.0971 - val_accuracy: 0.9850\n",
            "Epoch 60/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0080 - accuracy: 0.9966 - val_loss: 0.0763 - val_accuracy: 0.9900\n",
            "Epoch 61/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0098 - accuracy: 0.9974 - val_loss: 0.0737 - val_accuracy: 0.9850\n",
            "Epoch 62/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0077 - accuracy: 0.9983 - val_loss: 0.0924 - val_accuracy: 0.9850\n",
            "Epoch 63/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0093 - accuracy: 0.9963 - val_loss: 0.0712 - val_accuracy: 0.9900\n",
            "Epoch 64/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0079 - accuracy: 0.9977 - val_loss: 0.0597 - val_accuracy: 0.9900\n",
            "Epoch 65/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0055 - accuracy: 0.9980 - val_loss: 0.0886 - val_accuracy: 0.9900\n",
            "Epoch 66/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0090 - accuracy: 0.9966 - val_loss: 0.0819 - val_accuracy: 0.9900\n",
            "Epoch 67/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0055 - accuracy: 0.9977 - val_loss: 0.0608 - val_accuracy: 0.9850\n",
            "Epoch 68/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0074 - accuracy: 0.9980 - val_loss: 0.0814 - val_accuracy: 0.9950\n",
            "Epoch 69/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0082 - accuracy: 0.9974 - val_loss: 0.0670 - val_accuracy: 0.9850\n",
            "Epoch 70/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.9971 - val_loss: 0.0986 - val_accuracy: 0.9800\n",
            "Epoch 71/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0067 - accuracy: 0.9971 - val_loss: 0.0947 - val_accuracy: 0.9900\n",
            "Epoch 72/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0080 - accuracy: 0.9974 - val_loss: 0.1022 - val_accuracy: 0.9750\n",
            "Epoch 73/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0062 - accuracy: 0.9980 - val_loss: 0.0555 - val_accuracy: 0.9900\n",
            "Epoch 74/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0052 - accuracy: 0.9986 - val_loss: 0.0929 - val_accuracy: 0.9850\n",
            "Epoch 75/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0053 - accuracy: 0.9983 - val_loss: 0.0833 - val_accuracy: 0.9850\n",
            "Epoch 76/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0037 - accuracy: 0.9994 - val_loss: 0.0751 - val_accuracy: 0.9850\n",
            "Epoch 77/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0066 - accuracy: 0.9969 - val_loss: 0.1013 - val_accuracy: 0.9850\n",
            "Epoch 78/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0067 - accuracy: 0.9971 - val_loss: 0.1083 - val_accuracy: 0.9850\n",
            "Epoch 79/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0067 - accuracy: 0.9969 - val_loss: 0.1145 - val_accuracy: 0.9850\n",
            "Epoch 80/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0054 - accuracy: 0.9983 - val_loss: 0.1056 - val_accuracy: 0.9850\n",
            "Epoch 81/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0075 - accuracy: 0.9977 - val_loss: 0.1334 - val_accuracy: 0.9800\n",
            "Epoch 82/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0076 - accuracy: 0.9969 - val_loss: 0.1267 - val_accuracy: 0.9900\n",
            "Epoch 83/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0088 - accuracy: 0.9974 - val_loss: 0.1294 - val_accuracy: 0.9800\n",
            "Epoch 84/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0062 - accuracy: 0.9980 - val_loss: 0.1115 - val_accuracy: 0.9750\n",
            "Epoch 85/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0084 - accuracy: 0.9969 - val_loss: 0.0836 - val_accuracy: 0.9900\n",
            "Epoch 86/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.0967 - val_accuracy: 0.9900\n",
            "Epoch 87/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0044 - accuracy: 0.9980 - val_loss: 0.0918 - val_accuracy: 0.9900\n",
            "Epoch 88/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0049 - accuracy: 0.9989 - val_loss: 0.0969 - val_accuracy: 0.9900\n",
            "Epoch 89/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0126 - accuracy: 0.9954 - val_loss: 0.1421 - val_accuracy: 0.9850\n",
            "Epoch 90/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0112 - accuracy: 0.9966 - val_loss: 0.0535 - val_accuracy: 0.9900\n",
            "Epoch 91/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0151 - accuracy: 0.9957 - val_loss: 0.0637 - val_accuracy: 0.9850\n",
            "Epoch 92/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0145 - accuracy: 0.9946 - val_loss: 0.1742 - val_accuracy: 0.9750\n",
            "Epoch 93/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0079 - accuracy: 0.9971 - val_loss: 0.0668 - val_accuracy: 0.9900\n",
            "Epoch 94/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0064 - accuracy: 0.9983 - val_loss: 0.0790 - val_accuracy: 0.9900\n",
            "Epoch 95/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0063 - accuracy: 0.9971 - val_loss: 0.0970 - val_accuracy: 0.9850\n",
            "Epoch 96/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0061 - accuracy: 0.9980 - val_loss: 0.1049 - val_accuracy: 0.9850\n",
            "Epoch 97/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0056 - accuracy: 0.9977 - val_loss: 0.1053 - val_accuracy: 0.9850\n",
            "Epoch 98/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0039 - accuracy: 0.9986 - val_loss: 0.1354 - val_accuracy: 0.9850\n",
            "Epoch 99/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0074 - accuracy: 0.9969 - val_loss: 0.0778 - val_accuracy: 0.9850\n",
            "Epoch 100/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0650 - val_accuracy: 0.9900\n",
            "Epoch 101/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0047 - accuracy: 0.9980 - val_loss: 0.0965 - val_accuracy: 0.9900\n",
            "Epoch 102/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0033 - accuracy: 0.9986 - val_loss: 0.1011 - val_accuracy: 0.9850\n",
            "Epoch 103/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0036 - accuracy: 0.9986 - val_loss: 0.0928 - val_accuracy: 0.9800\n",
            "Epoch 104/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0042 - accuracy: 0.9989 - val_loss: 0.0881 - val_accuracy: 0.9900\n",
            "Epoch 105/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0032 - accuracy: 0.9983 - val_loss: 0.1198 - val_accuracy: 0.9850\n",
            "Epoch 106/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0037 - accuracy: 0.9983 - val_loss: 0.0946 - val_accuracy: 0.9850\n",
            "Epoch 107/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0037 - accuracy: 0.9986 - val_loss: 0.1250 - val_accuracy: 0.9850\n",
            "Epoch 108/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0059 - accuracy: 0.9980 - val_loss: 0.1017 - val_accuracy: 0.9800\n",
            "Epoch 109/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0062 - accuracy: 0.9977 - val_loss: 0.0960 - val_accuracy: 0.9900\n",
            "Epoch 110/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0051 - accuracy: 0.9986 - val_loss: 0.0744 - val_accuracy: 0.9900\n",
            "Epoch 111/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0039 - accuracy: 0.9986 - val_loss: 0.0921 - val_accuracy: 0.9850\n",
            "Epoch 112/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0060 - accuracy: 0.9977 - val_loss: 0.0809 - val_accuracy: 0.9900\n",
            "Epoch 113/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0034 - accuracy: 0.9986 - val_loss: 0.0867 - val_accuracy: 0.9850\n",
            "Epoch 114/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0057 - accuracy: 0.9980 - val_loss: 0.0736 - val_accuracy: 0.9900\n",
            "Epoch 115/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.9974 - val_loss: 0.0659 - val_accuracy: 0.9900\n",
            "Epoch 116/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0057 - accuracy: 0.9971 - val_loss: 0.0377 - val_accuracy: 0.9900\n",
            "Epoch 117/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0066 - accuracy: 0.9977 - val_loss: 0.0575 - val_accuracy: 0.9900\n",
            "Epoch 118/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0042 - accuracy: 0.9994 - val_loss: 0.0599 - val_accuracy: 0.9850\n",
            "Epoch 119/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0062 - accuracy: 0.9971 - val_loss: 0.0583 - val_accuracy: 0.9900\n",
            "Epoch 120/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0042 - accuracy: 0.9983 - val_loss: 0.0474 - val_accuracy: 0.9900\n",
            "Epoch 121/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0024 - accuracy: 0.9997 - val_loss: 0.0477 - val_accuracy: 0.9900\n",
            "Epoch 122/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0045 - accuracy: 0.9980 - val_loss: 0.0720 - val_accuracy: 0.9900\n",
            "Epoch 123/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0042 - accuracy: 0.9986 - val_loss: 0.0876 - val_accuracy: 0.9850\n",
            "Epoch 124/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0024 - accuracy: 0.9997 - val_loss: 0.0905 - val_accuracy: 0.9900\n",
            "Epoch 125/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0044 - accuracy: 0.9986 - val_loss: 0.0971 - val_accuracy: 0.9900\n",
            "Epoch 126/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0031 - accuracy: 0.9989 - val_loss: 0.0712 - val_accuracy: 0.9900\n",
            "Epoch 127/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0032 - accuracy: 0.9989 - val_loss: 0.0691 - val_accuracy: 0.9900\n",
            "Epoch 128/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0976 - val_accuracy: 0.9800\n",
            "Epoch 129/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0077 - accuracy: 0.9971 - val_loss: 0.0558 - val_accuracy: 0.9900\n",
            "Epoch 130/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0064 - accuracy: 0.9974 - val_loss: 0.0414 - val_accuracy: 0.9900\n",
            "Epoch 131/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0071 - accuracy: 0.9974 - val_loss: 0.0776 - val_accuracy: 0.9850\n",
            "Epoch 132/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0122 - accuracy: 0.9966 - val_loss: 0.0449 - val_accuracy: 0.9850\n",
            "Epoch 133/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0039 - accuracy: 0.9989 - val_loss: 0.0608 - val_accuracy: 0.9900\n",
            "Epoch 134/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0674 - val_accuracy: 0.9850\n",
            "Epoch 135/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0045 - accuracy: 0.9980 - val_loss: 0.0638 - val_accuracy: 0.9850\n",
            "Epoch 136/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.0624 - val_accuracy: 0.9850\n",
            "Epoch 137/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0034 - accuracy: 0.9986 - val_loss: 0.0649 - val_accuracy: 0.9850\n",
            "Epoch 138/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.0520 - val_accuracy: 0.9900\n",
            "Epoch 139/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0050 - accuracy: 0.9977 - val_loss: 0.1058 - val_accuracy: 0.9800\n",
            "Epoch 140/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0056 - accuracy: 0.9983 - val_loss: 0.1152 - val_accuracy: 0.9800\n",
            "Epoch 141/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0083 - accuracy: 0.9969 - val_loss: 0.0547 - val_accuracy: 0.9850\n",
            "Epoch 142/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0066 - accuracy: 0.9977 - val_loss: 0.1266 - val_accuracy: 0.9800\n",
            "Epoch 143/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0070 - accuracy: 0.9969 - val_loss: 0.1099 - val_accuracy: 0.9850\n",
            "Epoch 144/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0044 - accuracy: 0.9986 - val_loss: 0.1131 - val_accuracy: 0.9750\n",
            "Epoch 145/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0059 - accuracy: 0.9989 - val_loss: 0.0619 - val_accuracy: 0.9850\n",
            "Epoch 146/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0070 - accuracy: 0.9974 - val_loss: 0.1171 - val_accuracy: 0.9850\n",
            "Epoch 147/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0080 - accuracy: 0.9983 - val_loss: 0.1304 - val_accuracy: 0.9800\n",
            "Epoch 148/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0077 - accuracy: 0.9977 - val_loss: 0.0676 - val_accuracy: 0.9850\n",
            "Epoch 149/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0063 - accuracy: 0.9980 - val_loss: 0.0882 - val_accuracy: 0.9850\n",
            "Epoch 150/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0065 - accuracy: 0.9983 - val_loss: 0.0877 - val_accuracy: 0.9850\n",
            "Epoch 151/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0050 - accuracy: 0.9983 - val_loss: 0.0825 - val_accuracy: 0.9800\n",
            "Epoch 152/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0028 - accuracy: 0.9991 - val_loss: 0.0936 - val_accuracy: 0.9800\n",
            "Epoch 153/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0040 - accuracy: 0.9989 - val_loss: 0.0889 - val_accuracy: 0.9800\n",
            "Epoch 154/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.1180 - val_accuracy: 0.9800\n",
            "Epoch 155/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0036 - accuracy: 0.9986 - val_loss: 0.1279 - val_accuracy: 0.9750\n",
            "Epoch 156/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0021 - accuracy: 0.9991 - val_loss: 0.1113 - val_accuracy: 0.9800\n",
            "Epoch 157/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0049 - accuracy: 0.9980 - val_loss: 0.1376 - val_accuracy: 0.9800\n",
            "Epoch 158/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0061 - accuracy: 0.9983 - val_loss: 0.1448 - val_accuracy: 0.9850\n",
            "Epoch 159/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0027 - accuracy: 0.9989 - val_loss: 0.1206 - val_accuracy: 0.9800\n",
            "Epoch 160/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0028 - accuracy: 0.9986 - val_loss: 0.0973 - val_accuracy: 0.9850\n",
            "Epoch 161/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.0956 - val_accuracy: 0.9850\n",
            "Epoch 162/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.9974 - val_loss: 0.1957 - val_accuracy: 0.9750\n",
            "Epoch 163/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0113 - accuracy: 0.9954 - val_loss: 0.0825 - val_accuracy: 0.9850\n",
            "Epoch 164/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0056 - accuracy: 0.9974 - val_loss: 0.1222 - val_accuracy: 0.9850\n",
            "Epoch 165/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0070 - accuracy: 0.9969 - val_loss: 0.0762 - val_accuracy: 0.9900\n",
            "Epoch 166/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0065 - accuracy: 0.9977 - val_loss: 0.0957 - val_accuracy: 0.9900\n",
            "Epoch 167/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0040 - accuracy: 0.9989 - val_loss: 0.1039 - val_accuracy: 0.9900\n",
            "Epoch 168/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0038 - accuracy: 0.9986 - val_loss: 0.0921 - val_accuracy: 0.9900\n",
            "Epoch 169/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.1020 - val_accuracy: 0.9850\n",
            "Epoch 170/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0015 - accuracy: 0.9994 - val_loss: 0.0919 - val_accuracy: 0.9900\n",
            "Epoch 171/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.0902 - val_accuracy: 0.9850\n",
            "Epoch 172/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0042 - accuracy: 0.9980 - val_loss: 0.0941 - val_accuracy: 0.9850\n",
            "Epoch 173/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.0968 - val_accuracy: 0.9850\n",
            "Epoch 174/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0024 - accuracy: 0.9991 - val_loss: 0.0999 - val_accuracy: 0.9900\n",
            "Epoch 175/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0035 - accuracy: 0.9989 - val_loss: 0.0969 - val_accuracy: 0.9850\n",
            "Epoch 176/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0025 - accuracy: 0.9991 - val_loss: 0.1604 - val_accuracy: 0.9850\n",
            "Epoch 177/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0029 - accuracy: 0.9986 - val_loss: 0.1227 - val_accuracy: 0.9900\n",
            "Epoch 178/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0043 - accuracy: 0.9983 - val_loss: 0.1152 - val_accuracy: 0.9850\n",
            "Epoch 179/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0044 - accuracy: 0.9989 - val_loss: 0.1157 - val_accuracy: 0.9800\n",
            "Epoch 180/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0061 - accuracy: 0.9983 - val_loss: 0.0942 - val_accuracy: 0.9850\n",
            "Epoch 181/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0052 - accuracy: 0.9977 - val_loss: 0.1339 - val_accuracy: 0.9850\n",
            "Epoch 182/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0037 - accuracy: 0.9991 - val_loss: 0.1650 - val_accuracy: 0.9800\n",
            "Epoch 183/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0040 - accuracy: 0.9991 - val_loss: 0.1382 - val_accuracy: 0.9850\n",
            "Epoch 184/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0039 - accuracy: 0.9991 - val_loss: 0.1716 - val_accuracy: 0.9800\n",
            "Epoch 185/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0031 - accuracy: 0.9989 - val_loss: 0.1493 - val_accuracy: 0.9850\n",
            "Epoch 186/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0123 - accuracy: 0.9969 - val_loss: 0.2286 - val_accuracy: 0.9800\n",
            "Epoch 187/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0094 - accuracy: 0.9960 - val_loss: 0.1243 - val_accuracy: 0.9800\n",
            "Epoch 188/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0093 - accuracy: 0.9969 - val_loss: 0.1489 - val_accuracy: 0.9850\n",
            "Epoch 189/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0062 - accuracy: 0.9983 - val_loss: 0.2177 - val_accuracy: 0.9700\n",
            "Epoch 190/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0048 - accuracy: 0.9989 - val_loss: 0.1697 - val_accuracy: 0.9800\n",
            "Epoch 191/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0039 - accuracy: 0.9991 - val_loss: 0.1402 - val_accuracy: 0.9800\n",
            "Epoch 192/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0055 - accuracy: 0.9983 - val_loss: 0.1842 - val_accuracy: 0.9800\n",
            "Epoch 193/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0063 - accuracy: 0.9980 - val_loss: 0.1358 - val_accuracy: 0.9850\n",
            "Epoch 194/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.9980 - val_loss: 0.1831 - val_accuracy: 0.9850\n",
            "Epoch 195/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0049 - accuracy: 0.9986 - val_loss: 0.1721 - val_accuracy: 0.9850\n",
            "Epoch 196/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0043 - accuracy: 0.9986 - val_loss: 0.1719 - val_accuracy: 0.9850\n",
            "Epoch 197/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0043 - accuracy: 0.9980 - val_loss: 0.1741 - val_accuracy: 0.9850\n",
            "Epoch 198/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0067 - accuracy: 0.9977 - val_loss: 0.1650 - val_accuracy: 0.9850\n",
            "Epoch 199/200\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.0040 - accuracy: 0.9989 - val_loss: 0.1693 - val_accuracy: 0.9800\n",
            "Epoch 200/200\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.0048 - accuracy: 0.9983 - val_loss: 0.1708 - val_accuracy: 0.9750\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGA66LeXMwK2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "outputId": "b4312834-2435-44c8-f9fd-e349505eddab"
      },
      "source": [
        "# initialize the learning rate finder and then train with learning\n",
        "lrf = LearningRateFinder(model)\n",
        "lrf.find((x_train, y_train),1e-10, 1e+1)\n",
        "# plot the loss for the various learning rates and save the\n",
        "# resulting plot to disk\n",
        "lrf.plot_loss()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/19\n",
            "110/110 [==============================] - 0s 4ms/step - loss: 0.0037 - accuracy: 0.9983\n",
            "Epoch 2/19\n",
            "110/110 [==============================] - 0s 4ms/step - loss: 0.0059 - accuracy: 0.9974\n",
            "Epoch 3/19\n",
            "110/110 [==============================] - 0s 4ms/step - loss: 0.0054 - accuracy: 0.9969\n",
            "Epoch 4/19\n",
            "110/110 [==============================] - 0s 4ms/step - loss: 0.0042 - accuracy: 0.9983\n",
            "Epoch 5/19\n",
            "110/110 [==============================] - 0s 4ms/step - loss: 0.0074 - accuracy: 0.9980\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}